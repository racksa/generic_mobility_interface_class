# Compiling the code
Each machine the code is designed to run on has a custom entry in the makefile of the form `cilia_XXX` that should be used to generate the appropriate executable. In general, the code should be compiled by calling

`make cilia_clean cilia_XXX`

# Running the code
**On a free-use machine**: The GPUs on which the code should run must be set using the environment variable `CUDA_VISIBLE_DEVICES` in `run_cilia_sim.sh`. The GPU identifiers do not need to be in ascending order. The code can then be run by calling `./run_cilia_sim.sh`, although it may need to be assigned the correct permissions first (as discussed inside the script itself).

**On Imperial HPC using the PBS queue system**: The execution will be allocated specific GPUs to use in accordance with the requested resources for the job; these GPUs will be defined by `CUDA_VISIBLE_DEVICES`. The request for resources and eventual execution of the code are done by modifying the bash script `run_cilia_sim.pbs` and then submitting `qsub run_cilia_sim.pbs` for the free queue or `qsub -q express -P exp-12345 run_cilia_sim.pbs`, with `12345` replaced by the express account code, for the express queue. This script will automatically move all data produced by the simulation into a folder named `<PBS_JOB_ID_NUMBER>.output`, but will not move the error and console output files -- `run_cilia_sim.pbs.e<PBS_JOB_ID_NUMBER>` and `run_cilia_sim.pbs.o<PBS_JOB_ID_NUMBER>` -- because these don't seem to be produced until the script returns. *Note*: For the free queue, you can provide a realistic estimate for the resources required and the queue system will allocate you a node with atleast enough resourses, but for the express queue you must request resources that match one of classes they provide; e.g. to run on a 2-GPU node, you must also ask for 8 CPUs and 48GB of memory, whether you need it or not. This is how the free queue used to work.
